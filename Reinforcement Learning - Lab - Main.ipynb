{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this lab you will take what you have learned about Reinforcement Learning and build an AI capable of playing a game. That game is called CartPole and involves balancing a pole that is on a cart.\n",
    "<br>\n",
    "<br>\n",
    "More info about the environment can be found here\n",
    "<br>\n",
    "http://gym.openai.com/envs/CartPole-v1/\n",
    "<br>\n",
    "<br>\n",
    "This is a great first environment to try out RL on because it is siginificantly less expensive to train than other environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "You will be able to:\n",
    "* Build a DQN agent\n",
    "* Train the agent to play CartPole, achieving an average score of at least 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries\n",
    "Let's start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from collections import deque\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up environment\n",
    "Now we need to create the environment. The maximum number of steps in the game is by default set to 200. For this lab we will want to achieve an average score of 500, so we need to increase the max episode steps to 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "env._max_episode_steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the size of the action space by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that there are 2 possible actions - 0 and 1. In this environment 0 correspondings to the cart moving left and 1 corresponds to the cart moving right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows the observation space for CartPole. This is the data we will get for each state of the game. These 4 variables will be what we use to train our model. It is important to note that these values will be normalized to [0,1] before they are given to us in the state data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -24 deg        24 deg\n",
    "        3\tPole Velocity At Tip      -Inf            Inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some addtional info can be seen below regarding how the game decides it has lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Termination: \n",
    "        Pole Angle is more than 12 degrees\n",
    "        Cart Position is more than 2.4 (center of the cart reaches the edge of the display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring in this game is simple - each step that is made without ending the game will grant +1 point. A step that terminates the game will grant -1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the environment up and running and have it play a couple games randomly so we can get an idea of the baseline performance. We will keep track of the score each game so that we can get an average per 100 games. We will also plot the scores of each game to get idea of the variation in scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 100\n",
    "n_steps = 1000\n",
    "scores = []\n",
    "for game in range(n_games):\n",
    "    state = env.reset()\n",
    "    for t in range(n_steps):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Game ' + str(game) + ' score: ' + str(t))\n",
    "            scores.append(t)\n",
    "            break\n",
    "env.close()\n",
    "plt.plot(scores)\n",
    "plt.title('End Game Scores')\n",
    "plt.xlabel('Game')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our average score per 100 games is then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a DQN Agent\n",
    "Now we can start building the agent that will learn to play CartPole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want to choose and initialize our parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each state we are given 4 values, thus the input_shape for our network will be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch_size will be 64. This means that every time we perform experience replay, we will sample 64 different memories and train the model on those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set the total number of games to 2000. Almost certainly the model will reach its goal performance after less games (~500-1000) but just in case it takes longer we will extend the maximum number of games.\n",
    "<br>\n",
    "<br>\n",
    "The number of steps will be set to 1000 which is the maxmimum number of steps we set initially.\n",
    "<br>\n",
    "<br>\n",
    "We can also declare our score goal of 500 (avg per 100 games)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_games = 2000\n",
    "n_steps = 1000\n",
    "score_goal = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our epsilon greedy policy, we will start epsilon off at 1 and decay it by 0.95 after each training session. This way, over time, the network will rely more and more on its own predictions for moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1\n",
    "epsilon_decay = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma is our reward discount. This lessens the value of future rewards when calculating Q-values. The value is 0.95 is the most common value used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to initialize our memory. We will use an object known as a deque. A deque is a lot like a normal list, except that it has a max length and once that max length is reached, a new item added to the deque will cause the oldest item in the deque to be deleted. That way we only keep a maximum number of memories in storage at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "Now let's build and intial both our primary model and target model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This architecture was found mostly through trial and error. Feel free to modify it. Odds are there is a better architecture that will train the model faster.\n",
    "<br>\n",
    "<br>\n",
    "Note that our input_dim is equal to our input_shape - 4. Also, the final layer of the network has 2 neurons - corresponding to our two possible actions: left or right. It is essential to use a linear activation function at the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=input_shape, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the same architecture to create our target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = Sequential()\n",
    "target_model.add(Dense(16, input_dim=input_shape, activation='relu'))\n",
    "target_model.add(Dense(32, activation='relu'))\n",
    "target_model.add(Dense(16, activation='relu'))\n",
    "target_model.add(Dense(8, activation='relu'))\n",
    "target_model.add(Dense(2, activation='linear'))\n",
    "target_model.compile(loss=\"mse\", optimizer=Adam(lr=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Methods\n",
    "In order to train our models, we need to set up the functions required for a DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two methods will help our agent choose an action. <br><br>\n",
    "<b>choose_action</b> will generate a random number, if it is less than our current epsilon value, then a random move will be chosen. If the random number is greater than our current epsilon value, we will need to predict a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    if np.random.rand() <= epsilon:    \n",
    "        action = random.choice([0,1])\n",
    "    else:\n",
    "        action = predict_action(state)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>predict_action</b> will take in a state and predict the Q-values for that state for each action. Then we can use argmax() to get the action with the highest Q-value and return that action.\n",
    "<br>\n",
    "<br>\n",
    "Note that we are using the target model to predict here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(state):\n",
    "    q_values = target_model.predict(state)\n",
    "    action = np.argmax(q_values[0])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create a function that will store the agent's experiences into memory. The function is fairly straightforward, we will simply pass in the current state, the performed action, the reward, the next state, and the 'done' flag and append those values as a tuple into our memory deque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two additional helper functions - one to decay our epsilon value and one to transfer the weights of the primary model to the target model. These are both fairly straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrease_epsilon():\n",
    "    global epsilon\n",
    "    epsilon *= epsilon_decay\n",
    "    \n",
    "def transfer_weights():\n",
    "    target_model.set_weights(model.get_weights()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for probably the most complicated of the methods - experience replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way this method works is, first we check and make sure we have enough experience in memory to create a batch of our selected batch_size.\n",
    "<br>\n",
    "<br>\n",
    "Next we create our batches by randomly sampling our memory a number of times equal to batch_size.\n",
    "<br>\n",
    "<br>\n",
    "Then we iterate through each batch. For each batch, we calculate the updated Q-value. The updated Q-value is calculated by using the Q-value formula. <br><br>\n",
    "     <img src ='q_formula.png'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this formula, the Q-value is calculated by adding together the current reward and the maximum possible Q-value of the next state, discount by our gamma value.\n",
    "<br><br>\n",
    "Note that we are using the target model to make this prediction.\n",
    "<br><br>\n",
    "Next we want to grab the current predicted Q-values for the given state.\n",
    "<br>\n",
    "<br>\n",
    "We then update only the Q-value for the chosen action, leaving the other Q-value the same.\n",
    "<br>\n",
    "<br>\n",
    "Our primary model is then fitted on the state and the Q-values.\n",
    "<br>\n",
    "<br>\n",
    "Lastly, after each training session, we want to decay our epsilon value and also transfer the weights from the primary model to the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    print('**training**')\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in batch:\n",
    "        q_update = reward\n",
    "        if not done:\n",
    "            q_update = (reward + gamma * np.amax(target_model.predict(next_state)[0]))\n",
    "        q_values = target_model.predict(state)\n",
    "        q_values[0][action] = q_update\n",
    "        model.fit(state, q_values, verbose=0)\n",
    "    decrease_epsilon() \n",
    "    transfer_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up training, we can adjust one of our input values - cart position - to force it into bins. This will help to shrink our overall state size. This works because the agent will still be learning the position of the cart on the rail, it will just be given more of a general position instead of a very specific position.\n",
    "<br>\n",
    "<br>\n",
    "You don't have to worry too much about how exactly this code works, just know that each state needs to pass through the <b>cart_position_to_bin</b> function to adjust the cart position values.\n",
    "<br>\n",
    "<br>\n",
    "Specifically we will be create 5 bins of cart positions. Feel free to experiment with more or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 5\n",
    "bin_size = 1/n_bins\n",
    "bins = []\n",
    "for i in range(n_bins):\n",
    "    bins.append((0, bin_size*i, i))\n",
    "\n",
    "def cart_position_to_bin(state):\n",
    "    position = state[0]\n",
    "    for b in bins:\n",
    "        if position>=b[0] and position<=b[1]:\n",
    "            binned_position = b[2]\n",
    "            state[0] = binned_position\n",
    "            break\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get our state to be the correct size for the neural network, we also need to reshape the array. Below is a method to do this, coupled with the above cart position function to give you one state preprocessing method. Make sure to pass each state and next_state through this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    state = cart_position_to_bin(state)\n",
    "    state = np.reshape(state, [1, input_shape])\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the prework is now complete. We have all the tools necessary to start training our agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "Alright! Now it's time to start training. To do this, we will need to build a loop that will play through games of CartPole, training our agent along the way.\n",
    "<br><br>\n",
    "Provided below is a basic skeleton of this loop. Use what you have learned from the lesson and this lab in order to complete it. If you get lost you can always check the solution.\n",
    "<br><br>\n",
    "(Warning: this process may take some time. It takes anywhere from 30-60 minutes on my machine but that may be more or less depending on your computer and GPU.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "scores = []\n",
    "avgs = []\n",
    "for game in range(n_games):\n",
    "    ## Each game, we need to reset the environment, grab the intial state and preprocess it\n",
    "    ##### do stuff here #####\n",
    "    for t in range(n_steps):\n",
    "        ## After each move, we need to do the following:\n",
    "        ## 1) render the environment\n",
    "        ## 2) decide on an action for this turn\n",
    "        ## 3) perform that action, grabbing the next state, reward, 'done' flag, and 'info' object\n",
    "        ## 4) preprocess the next state\n",
    "        ## 5) remember this experience\n",
    "        ## 6) set state=next_state to prepare for the next round\n",
    "        ## 7) check if game is done. if so, record the score,\n",
    "        ##    calculate the rolling average score (average score per last 100 games),\n",
    "        ##    store the rolling average\n",
    "        ## 8) check if average score goal has been reached, if so break out of game loop\n",
    "        ## 9) if we need to keep playing, perform experience replay\n",
    "        ##### do stuff here #####\n",
    "        if done:\n",
    "            ##### do stuff here #####\n",
    "            print (\"Game: \" + str(game) + \", exploration: \" + str(round(epsilon,2)) + \", score: \" + str(t) + ' - avg score/100 games: ' + str(avg_100))\n",
    "            break\n",
    "    if avg_100 >= score_goal:\n",
    "        ##### do stuff here #####\n",
    "        print('Completed after ' + str(game) + ' games - Averaging over ' + str(score_goal) + 'points per 100 games')\n",
    "        break\n",
    "    ##### do stuff here #####\n",
    "env.close()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is finished, let's plot the rolling average scores to see how the agent improved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avgs)\n",
    "plt.title('Average score per 100 games')\n",
    "plt.xlabel('Games (x100)')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Congratulations! You just trained your first RL model! After completing the rite of passage that is CartPole. <br><br>\n",
    "You are now eligible to try out other environments. \n",
    "A word of caution though - the Atari games may be the most exciting and enticing environments to train in, but they may take several days of training before convergence. They also may involve image data and the use of Convolutional Neural Networks so make sure you are comfortable with those first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can do a quick comparison of baseline CartPole vs our trained CartPole agent.\n",
    "<br><br>\n",
    "These blocks will run both a random agent and our trained agent for 3 games. It also adds a small delay at each step so that you can watch both agents in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Agent\n",
    "n_games = 3\n",
    "n_steps = 1000\n",
    "scores = []\n",
    "for game in range(n_games):\n",
    "    state = env.reset()\n",
    "    for t in range(n_steps):\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('Game ' + str(game) + ' score: ' + str(t))\n",
    "            scores.append(t)\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DQN Agent\n",
    "n_games = 3\n",
    "n_steps = 1000\n",
    "scores = []\n",
    "for game in range(n_games):\n",
    "    state = env.reset()\n",
    "    state = preprocess_state(state)\n",
    "    for t in range(n_steps):\n",
    "        time.sleep(0.05)\n",
    "        env.render()\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = preprocess_state(next_state)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print('Game ' + str(game) + ' score: ' + str(t))\n",
    "            scores.append(t)\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
